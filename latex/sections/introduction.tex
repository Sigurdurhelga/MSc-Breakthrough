\chapter{Introduction\label{cha:introduction}}
%% \ifdraft only shows the text in the first argument if you are in draft mode.
%% These directions will disappear in other modes.

The world of deep learning (DL) is exciting, we have models that can examine
images and very reliably be able to identify it's content. Deep learning models have
beaten Ophthalmologists in identifying diabetic retinopathy, they've identified
cancer cells where others have not. Deep learning models are used for the bleeding
edge of protein folding, to gain further understanding of the underlying structure of organisms, such as, viruses. \cite{deepmind:alphafold}
These models have done these things extremely accurately, cheap, and fast.

DL in conjunction with a randomized State-space Search Algorithm Monte-Carlo Tree Search was used to defeat the standing world champion Lee Sedol (CITE ALPHAGO ZERO) in the incredibly expansive game of Go. This was done in the year 2017, the researchers used reinforcement learning (RL), where the agent learned by only playing against itself.
These results imply that given enough time to learn the computer agent can gain a deep insight into how the game should be played.

The field of examining deep learning models to gain a richer understanding
of how they work, and what makes them so much better than humans at various tasks
is still new. This is the field of Explainable Artificial Intelligence (XAI).
If we wish to continue using artificial intelligence (AI) and machine learning (ML),
to improve our lives we must examine how they work, this is not only to improve
the models themselves but could also augment our ability in many cases.
Furthermore, these explanations are a legal requirement now in many areas\cite{legalexplanation:goodman}, namely, legal, and
medical. Recently, XAI has generally been focused on Model Interpretability (MI), in
particular, interpreting the model in such a way that we might predict what the model will
do given a particular input. Another class of XAI would be Model Explainability (ME), where we can adequately explain how the internal mechanics of the model works.
A simple example of when we're able to apply ME would be when we print out a decision tree
that is generated from an ML algorithm, with the decision tree nodes and is conditions we can see exactly what the tree will do with a given input. While in MI we would have to predict the results, without going into details as to why.

In this thesis, we take a look at how we can examine an artificial neural network (ANN)
to understand which higher-level concepts (HLC) it deems important for a given state
within games. These games can be simple like Tic-Tac-Toe or Breakthrough and extremely
complicated like Chess or Go.

The method of examining HLC's within games has generally been done by examining the
current state of the game by evaluating them concerning some heuristic. A heuristic within games are evaluations of the end reward for a state given only the state, for example,
the number of pieces left within a game of chess. Intuitively, the amount of pieces
left is a good estimate for a state in chess, this is a higher-level concept we
use to evaluate a chess position. The piece amount can be considered as a lower-level concept
than other concepts we use. For instance, grandmaster chess players evaluate a position w.r.t. states
where the king is safe from attack or the structure of the pawn positions.
This paper examines the evaluation of a neural network of a state regarding those higher-level concepts if human players evaluate the king's safety of a state
low but the neural network highly values it, and the neural network plays better
than the player. There could be an avenue for us to improve our game by considering king safety more thoroughly.

We define the research questions:

\begin{enumerate}
  \item Given a NN that plays Breakthrough very well, can we evaluate if that NN recognizes the
        HLCs that human players use.
  \item Does a NN that trains itself using self-play learn some HLCs over time, and does it start to emphasize
        the HLCs that are generally considered better, more as it trains more, and oppositely does it start to
        demphasize HLCs that are considered worse.
\end{enumerate}

\subsection{Summary of the thesis}

In this thesis, we take an example game of Breakthrough, train a neural network to play the game using only self-play. That is, the neural network is trained only by playing against itself with no outside input other than the rules of the game itself. And we examine the neural networks against popular Higher-level concepts (heuristics) that we use to play the game.
