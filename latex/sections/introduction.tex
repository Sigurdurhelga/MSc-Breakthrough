\chapter{Introduction\label{cha:introduction}}
%% \ifdraft only shows the text in the first argument if you are in draft mode.
%% These directions will disappear in other modes.
\ifdraft{State the objectives of the exercise. Ask yourself:
  \underline{Why} did I design/create the item? What did I aim to
  achieve? What is the problem I am trying to solve?  How is my
  solution interesting or novel?}{}


The world of deep learning (DL) is exciting, we have models that are able to examine
images and very reliably be able to identify it's content. Deep learning models have
beaten Opthalmologists in identifing diabetic retinopathy, they've identified
cancer cells where others have not. Deep learning models have even predicted
the likelyhood a person will die in the following year with good accuracy (CITE).
These models have done these things extremely accurately, cheap, and fast.

DL in conjunction with a randomized State-space Search Algorithm Monte-Carlo Tree Search, 
was used to defeat the standing world champion Lee Sedol in the incredibly expansive game of Go. This was done in the year 
2017, the researchers used reinforcement learning (RL), where the agent learned by only playing against itself. 
These result imply that given enough time to learn the computer agent is able to
gain a deep insight on how the game should be played.  

The field of examining deep learning models to gain a richer understanding
of how they work, and what makes them so much better than humans at various tasks
is still new. This is the field of Explainable Artificial Intelligence (XAI).
If we wish to continue using artificial intelligence (AI) and machine learning (ML),
to improve our lives we must examine how they work, this is not only to improve
the models themselves but could also augment our ability in many cases.
Furthermore, these explanations are a legal requirement now in many areas, namely, legal, and
medical. Recently, XAI has generally been focused on Model Interpretability, in
particular focus on gaining insights on the concepts learned by Deep Neural Networks.

In this we thesis takes a look at how we can examine an artificial neural network (ANN)
to understand which higher level concepts (HLC) it deems important for a given state
within games. These games can be simple like Tic-Tac-Toe or Breakthrough and extremely
complicated like Chess or Go. 

The method of examining HLC's within games has generally been done by examining the
current state of the game by evaluating them with respect to some heuristic. A heuristic within
games are evaluations of the end reward for a state given only the state, for example,
the amount of pieces left within a game of chess. Intuitively, the amount of pieces
left is a good estimate for a state in chess, this is a higher level concept we
use to evaluate a chess position. The piece amount can be considered as a lower-level concept
than other concepts we use. For instance, grand master chess players evaluate a position w.r.t. states
where the king is safe from attack, or the structure of the pawn positions.
This paper examines a neural networks evaluation of a state regarding
those higher-level concepts, if human players evaluate the king safety of a state
low but the neural network highly values it, and the neural network plays better
than the player. There could be an avenue for us to improve our game by considering king safety more thoroughly.

We define the research questions

\begin{enumerate}
  \item Given a NN that plays Breakthrough very well, can we evaluate if that NN recognizes the 
  HLCs that human players use.
  \item Does a NN that trains itself using self-play learn these HLCs over time, and does it start to emphasize 
  the HLCs that are generally taught later to human players, more as it trains itself more.
  \item Does a NN that plays better than some humans focus on HLCs that are not generally 
  considered by those human players and it.
\end{enumerate}

\subsection{Summary of the thesis}

In this thesis we take an example game of Breakthrough, train a neural network to play the game
using only self-play. That is, the neural network is trained only by playing against
itself with no outside input other than the rules of the game itself. And we examine
the neural networks against popular Higher-level concepts (heuristics) that we use
to play the game.
