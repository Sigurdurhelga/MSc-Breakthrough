\chapter{Introduction\label{cha:introduction}}
%% \ifdraft only shows the text in the first argument if you are in draft mode.
%% These directions will disappear in other modes.

The world of deep learning (DL) is exciting. We have models that can examine images and reliably identify their content\cite{simonyan:imagerecognition}. Deep learning models have outperformed ophthalmologists in identifying diabetic retinopathy\cite{brown:retinopathy} and identified cancer cells where others have not\cite{kourou:cancer}. Deep learning models are used for the bleeding edge of protein folding, to gain further understanding of the underlying structure of organisms, such as viruses\cite{deepmind:alphafold}. These models have do these things accurately, cheap, and fast

DL in conjunction with a randomized state-space search algorithm, Monte-Carlo Tree Search, was used to defeat the standing world champion Lee Sedol in the incredibly expansive game of Go\cite{silver:alphagozero}. The researchers used reinforcement learning (RL), where the agent learned by only playing against itself. These results imply that given enough time to learn the computer agent can gain a deep insight into how a game should be played.

The field of examining deep learning models to gain a richer understanding of how they work, and what makes them so much better than humans at various tasks, is still new. This is the field of Explainable Artificial Intelligence (XAI). If we wish to continue using artificial intelligence (AI) and machine learning (ML) to improve our lives, we must examine how they work. Not only to improve the models themselves but also to augment our ability in many cases. Furthermore, these explanations are a legal requirement now in many areas\cite{legalexplanation:goodman}, namely, legal and medical. Recently, XAI has been focused on Model Interpretability (MI), in particular, interpreting the model in such a way that we can predict what the model will do given a particular input\cite{riberio:recentxai1}. Another class of XAI is Model Explainability (ME), where we attempt to adequately explain how the internal mechanics of the model works. A simple example of ME would be when we explore a decision tree generated by an ML algorithm. The decision tree nodes and conditions explain exactly what the tree will do with a given input. This differs from MI where we would have to predict the results, without going into details as to why.

In this thesis, we take a look at how we can examine an artificial neural network (ANN) to understand which higher-level concepts (HLC) it deems important for a given state within a game. Such a game can be simple like Tic-Tac-Toe or Breakthrough and complicated like Chess or Go.

The method of examining HLC's within games has generally been done by examining the current state of the game by evaluating them using some heuristic. A heuristic within games are evaluations of the expected end reward for a state, for example, by using the number and type of pieces left within a game of chess. Intuitively, the material advantage is a higher-level concept we can use to evaluate a chess position. Of course there are many other relevant concepts to consider, for example whether the king is safe from attack, and the pawn structure. This thesis examines the evaluation of a neural network of a given state with respect to those higher-level concepts. More precisely, we define the following research questions:

\begin{enumerate}
	\item \label{rq:1}Given a NN that plays Breakthrough, can we determine if that NN has learned the HLCs that human players use.
	\item \label{rq:2}Does a NN that trains itself using self-play learn some HLCs over time, and does it start to emphasize
	      the HLCs that are generally considered better the more it trains, and conversely, does it start to
	      demphasize HLCs that are considered worse.
\end{enumerate}

\subsection*{Summary}

In this thesis, we take an example game of Breakthrough, train a neural network to play the game using only self-play. That is, the neural network is trained only by playing against itself with no external input other than the rules of the game itself. And we examine the neural networks against popular higher-level concepts (heuristics) that we use to play the game.
